"""
Test suite for GR00T dependencies.

This module tests that GR00T dependencies are properly installed and functional:
- flash_attn package import and basic functionality
- Isaac-GR00T package import
"""

import importlib.util
import os
import sys

import pytest


class TestGr00tDependencies:
    """Test class for GR00T dependencies."""

    def test_flash_attn_import(self):
        """Test that flash_attn can be imported successfully."""
        try:
            import flash_attn  # pylint: disable=import-outside-toplevel

            # Verify version is available
            assert hasattr(flash_attn, "__version__"), "flash_attn version not available"
            print(f"Flash Attention version: {flash_attn.__version__}")
        except ImportError as e:
            pytest.fail(f"Failed to import flash_attn: {e}")

    def test_flash_attn_functionality(self):
        """Test basic flash_attn functionality."""
        pytest.importorskip("torch", reason="PyTorch not available")
        pytest.importorskip("flash_attn", reason="flash_attn not available")

        import torch  # pylint: disable=import-outside-toplevel

        from flash_attn import flash_attn_func  # pylint: disable=import-outside-toplevel

        # Skip test if CUDA is not available
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available for flash_attn test")

        try:
            # Create small test tensors
            batch_size, seq_len, num_heads, head_dim = 1, 32, 4, 64
            device = "cuda"
            dtype = torch.float16

            q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
            k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)
            v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)

            # Test flash attention function
            out = flash_attn_func(q, k, v)

            # Verify output shape
            expected_shape = (batch_size, seq_len, num_heads, head_dim)
            assert out.shape == expected_shape, f"Expected shape {expected_shape}, got {out.shape}"

            # Verify output is on correct device and dtype
            assert out.device.type == "cuda", f"Expected output on CUDA, got {out.device}"
            assert out.dtype == dtype, f"Expected dtype {dtype}, got {out.dtype}"

            print(f"Flash Attention test passed - output shape: {out.shape}")

        except (RuntimeError, ValueError) as e:
            pytest.fail(f"Flash Attention functionality test failed: {e}")
        finally:
            # Clean up GPU memory
            torch.cuda.empty_cache()

    def test_gr00t_package_directory_exists(self):
        """Test that GR00T package directory exists."""
        gr00t_path = "/workspaces/isaac_arena/submodules/Isaac-GR00T"

        assert os.path.exists(gr00t_path), f"GR00T directory not found at {gr00t_path}"
        assert os.path.isdir(gr00t_path), f"GR00T path exists but is not a directory: {gr00t_path}"

        print(f"GR00T package directory exists at: {gr00t_path}")

    def test_gr00t_package_import(self):
        """Test that GR00T package can be imported."""
        gr00t_path = "/workspaces/isaac_arena/submodules/Isaac-GR00T"

        # Add GR00T path to Python path if not already there
        if gr00t_path not in sys.path:
            sys.path.insert(0, gr00t_path)

        # If direct import fails, verify the directory has Python files
        python_files = []
        for root, _, files in os.walk(gr00t_path):
            for file in files:
                if file.endswith(".py") and not file.startswith("__"):
                    python_files.append(os.path.join(root, file))

        if python_files:
            print(f"GR00T directory contains {len(python_files)} Python files")
            # Try to import the first module we find as a basic test
            first_py_file = python_files[0]
            module_name = os.path.splitext(os.path.basename(first_py_file))[0]

            try:
                spec = importlib.util.spec_from_file_location(module_name, first_py_file)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    print(f"Successfully imported GR00T module: {module_name}")
                else:
                    pytest.fail("Could not create module spec from GR00T files")

            except (ImportError, AttributeError, ModuleNotFoundError) as e:
                pytest.fail(f"Failed to import any GR00T module: {e}")
        else:
            pytest.fail("No Python files found in GR00T directory")

    def test_pytorch_cuda_compatibility(self):
        """Test that PyTorch and CUDA are properly configured for GR00T."""
        pytest.importorskip("torch", reason="PyTorch not available")

        import torch  # pylint: disable=import-outside-toplevel

        # Check PyTorch version
        print(f"PyTorch version: {torch.__version__}")

        # Check CUDA availability
        if torch.cuda.is_available():
            print(f"CUDA available with {torch.cuda.device_count()} devices")
            if torch.cuda.device_count() > 0:
                print(f"Current device: {torch.cuda.get_device_name(0)}")

            # Test basic CUDA operations
            try:
                x = torch.randn(100, 100, device="cuda")
                y = torch.matmul(x, x.T)
                assert y.device.type == "cuda", "CUDA operation failed"
                print("Basic CUDA operations working")

                # Clean up
                del x, y
                torch.cuda.empty_cache()

            except (RuntimeError, AssertionError) as e:
                pytest.fail(f"CUDA operations failed: {e}")
        else:
            pytest.skip("CUDA not available, skipping CUDA compatibility test")


if __name__ == "__main__":
    # Run tests when script is executed directly
    pytest.main([__file__, "-v"])
